{"name":"Git-practical-machine-learning-report","tagline":"Coursera course (only R markdown and compiled HTML file describing analysis and Txt with the building comparison models)","body":"# _**Practical Machine Learning Report**_\r\n\r\n## **Abstract**\r\n\r\nThis report contains a small description about the exploration of models and techniques used for building the final model, how was realized the preprocess of the data and the selection and validation of the variables and estimation of the out sample error (cross validation)\r\n\r\n## **The Exploration of models**\r\n\r\nDuring the course, we saw different methods for building predictions; the exploration of models that fit best for our data and what been looking of prediction, starts with the random forests and trees techniques, thus because the lectures in the course and others lectures related with the theme indicates that these two methods are the most common and best suitable for classification problems like ours, and gives highest accuracy predictions.\r\n\r\n## **The Preprocess decision**\r\n\r\nThe first time that saw the training set given*, it's that there is some missing or non relative variables(registers), because some variables have no values at all, some others may present a value registered but not in all rows, and some others have NA values. So the first preprocess made was directly in the data source, eliminating those fields with this characteristics. The training set variables left with 58 of them, including the variable wich try to predict(classe).\r\nThe important preprocess technique made come later. Using principal components analysis (PCA) for picking those reamian variables that been correlated enough to bring us the best combination of significant predictors.\r\n\r\n## **The Cross validation and expected out of sample error**\r\n\r\nThe final model use the \"PCA\" method that give us the combination of the most important predictors that reduce the noise and maintain the accuracy. Combinating the sesgo of data training set and applying the PCA method to it come out whit 36 significant predictors of the total of 57 possible ones. The data training was split in 70% for training and 30% for testing.\r\n\r\nWhit all this, it was expected an out of sample error very small, (cause seeing no miss classification, will probably and surely would be wrong), estimating an error of 0.0042. In ej. with 20 predictions made we were wrong in 1.68 of them.\r\n\r\n## **Model and Confusion Matrix**\r\n\r\n                                  Random Forest\r\n\r\n13737 samples\r\n   57 predictors\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\nPre-processing: principal\r\n component signal extraction,\r\n scaled, centered \r\nResampling: Bootstrapped (25 reps) \r\n\r\nSummary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... \r\n\r\nResampling results across tuning parameters:\r\n\r\n         mtry Accuracy    Kappa    AccuracySD    KappaSD    \r\n         \r\n         2     0.974    0.967       0.00217      0.00273\r\n         \r\n         40    0.962    0.952       0.0052       0.00656\r\n         \r\n         79    0.962    0.952       0.00534      0.00674    \r\n\r\n  \r\nAccuracy was used to select\r\n the optimal model using  the\r\n largest value.\r\nThe final value used for the model\r\n was mtry = 2.\r\n\r\n\r\n                                  Confusion Matrix and Statistics\r\n\r\nPrediction    A    B    C    D    E\r\n\r\n         A 1671    5    0    0    0\r\n         \r\n         B    2 1133    6    0    0\r\n         \r\n         C    1    1 1020    8    0\r\n         \r\n         D    0    0    0  955    1\r\n         \r\n         E    0    0    0    1 1081\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9958 \r\n               \r\n                 95% CI : (0.9937, 0.9972)\r\n                 \r\n    No Information Rate : 0.2845          \r\n    \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9946\r\n                  \r\n Mcnemar's Test P-Value : NA \r\n\r\n\r\n*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. at [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}