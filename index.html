<!doctype html>
<!-- The Time Machine GitHub pages theme was designed and developed by Jon Rohan, on Feb 7, 2012. -->
<!-- Follow him for fun. http://twitter.com/jonrohan. Tail his code on http://github.com/jonrohan -->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <link rel="stylesheet" href="stylesheets/stylesheet.css" media="screen"/>
  <link rel="stylesheet" href="stylesheets/pygment_trac.css"/>
  <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
  <script type="text/javascript" src="javascripts/script.js"></script>

  <title>Git-practical-machine-learning-report</title>
  <meta name="description" content="Coursera course (only R markdown and compiled HTML file describing analysis and Txt with the building comparison models)">

  <meta name="viewport" content="width=device-width,initial-scale=1">

</head>

<body>

  <div class="wrapper">
    <header>
      <h1 class="title">Git-practical-machine-learning-report</h1>
    </header>
    <div id="container">
      <p class="tagline">Coursera course (only R markdown and compiled HTML file describing analysis and Txt with the building comparison models)</p>
      <div id="main" role="main">
        <div class="download-bar">
        <div class="inner">
          <a href="https://github.com/NpLiquid/Git-Practical-Machine-Learning-Report/tarball/master" class="download-button tar"><span>Download</span></a>
          <a href="https://github.com/NpLiquid/Git-Practical-Machine-Learning-Report/zipball/master" class="download-button zip"><span>Download</span></a>
          <a href="https://github.com/NpLiquid/Git-Practical-Machine-Learning-Report" class="code">View Git-practical-machine-learning-report on GitHub</a>
        </div>
        <span class="blc"></span><span class="trc"></span>
        </div>
        <article class="markdown-body">
          <h1>
<a name="practical-machine-learning-report" class="anchor" href="#practical-machine-learning-report"><span class="octicon octicon-link"></span></a><em><strong>Practical Machine Learning Report</strong></em>
</h1>

<h2>
<a name="abstract" class="anchor" href="#abstract"><span class="octicon octicon-link"></span></a><strong>Abstract</strong>
</h2>

<p>This report contains a small description about the exploration of models and techniques used for building the final model, how was realized the preprocess of the data and the selection and validation of the variables and estimation of the out sample error (cross validation)</p>

<h2>
<a name="the-exploration-of-models" class="anchor" href="#the-exploration-of-models"><span class="octicon octicon-link"></span></a><strong>The Exploration of models</strong>
</h2>

<p>During the course, we saw different methods for building predictions; the exploration of models that fit best for our data and what been looking of prediction, starts with the random forests and trees techniques, thus because the lectures in the course and others lectures related with the theme indicates that these two methods are the most common and best suitable for classification problems like ours, and gives highest accuracy predictions.</p>

<h2>
<a name="the-preprocess-decision" class="anchor" href="#the-preprocess-decision"><span class="octicon octicon-link"></span></a><strong>The Preprocess decision</strong>
</h2>

<p>The first time that saw the training set given*, it's that there is some missing or non relative variables(registers), because some variables have no values at all, some others may present a value registered but not in all rows, and some others have NA values. So the first preprocess made was directly in the data source, eliminating those fields with this characteristics. The training set variables left with 58 of them, including the variable wich try to predict(classe).
The important preprocess technique made come later. Using principal components analysis (PCA) for picking those reamian variables that been correlated enough to bring us the best combination of significant predictors.</p>

<h2>
<a name="the-cross-validation-and-expected-out-of-sample-error" class="anchor" href="#the-cross-validation-and-expected-out-of-sample-error"><span class="octicon octicon-link"></span></a><strong>The Cross validation and expected out of sample error</strong>
</h2>

<p>The final model use the "PCA" method that give us the combination of the most important predictors that reduce the noise and maintain the accuracy. Combinating the sesgo of data training set and applying the PCA method to it come out whit 36 significant predictors of the total of 57 possible ones. The data training was split in 70% for training and 30% for testing.</p>

<p>Whit all this, it was expected an out of sample error very small, (cause seeing no miss classification, will probably and surely would be wrong), estimating an error of 0.0042. In ej. with 20 predictions made we were wrong in 1.68 of them.</p>

<h2>
<a name="model-and-confusion-matrix" class="anchor" href="#model-and-confusion-matrix"><span class="octicon octicon-link"></span></a><strong>Model and Confusion Matrix</strong>
</h2>

<pre><code>                              Random Forest
</code></pre>

<p>13737 samples
   57 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' </p>

<p>Pre-processing: principal
 component signal extraction,
 scaled, centered 
Resampling: Bootstrapped (25 reps) </p>

<p>Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... </p>

<p>Resampling results across tuning parameters:</p>

<pre><code>     mtry Accuracy    Kappa    AccuracySD    KappaSD    

     2     0.974    0.967       0.00217      0.00273

     40    0.962    0.952       0.0052       0.00656

     79    0.962    0.952       0.00534      0.00674    
</code></pre>

<p>Accuracy was used to select
 the optimal model using  the
 largest value.
The final value used for the model
 was mtry = 2.</p>

<pre><code>                              Confusion Matrix and Statistics
</code></pre>

<p>Prediction    A    B    C    D    E</p>

<pre><code>     A 1671    5    0    0    0

     B    2 1133    6    0    0

     C    1    1 1020    8    0

     D    0    0    0  955    1

     E    0    0    0    1 1081
</code></pre>

<p>Overall Statistics</p>

<pre><code>           Accuracy : 0.9958 

             95% CI : (0.9937, 0.9972)

No Information Rate : 0.2845          

P-Value [Acc &gt; NIR] : &lt; 2.2e-16       

              Kappa : 0.9946
</code></pre>

<p>Mcnemar's Test P-Value : NA </p>

<p>*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. at <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a></p>
        </article>
      </div>
    </div>
    <footer>
      <div class="owner">
      <p><a href="https://github.com/NpLiquid" class="avatar"><img src="https://avatars0.githubusercontent.com/u/7957488?s=60" width="48" height="48"/></a> <a href="https://github.com/NpLiquid">NpLiquid</a> maintains <a href="https://github.com/NpLiquid/Git-Practical-Machine-Learning-Report">Git-practical-machine-learning-report</a></p>


      </div>
      <div class="creds">
        <small>This page generated using <a href="http://pages.github.com/">GitHub Pages</a><br/>theme by <a href="https://twitter.com/jonrohan/">Jon Rohan</a></small>
      </div>
    </footer>
  </div>
  <div class="current-section">
    <a href="#top">Scroll to top</a>
    <a href="https://github.com/NpLiquid/Git-Practical-Machine-Learning-Report/tarball/master" class="tar">tar</a><a href="https://github.com/NpLiquid/Git-Practical-Machine-Learning-Report/zipball/master" class="zip">zip</a><a href="" class="code">source code</a>
    <p class="name"></p>
  </div>

  
</body>
</html>
