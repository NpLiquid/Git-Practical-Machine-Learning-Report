---
ptitle: "Practical Machine Learning Report"
author: "A Yair"
date: "Monday, June 16, 2014"
output: html_document
---
Practical Machine Learning Report
----------------------------------
----------------------------------
**Abstract**

This report contains a small description about the exploration of models and techniques used for building the final model, how was realized the preprocess of the data and the selection and validation of the variables and estimation of the out sample error (cross validation)

**The Exploration of models**

During the course, we saw different methods for building predictions; the exploration of models that fit best for our data and what been looking of prediction, starts with the random forests and trees techniques, thus because the lectures in the course and others lectures related with the theme indicates that these two methods are the most common and best suitable for classification problems like ours, and gives highest accuracy predictions.

**The Preprocess decision**

The first time that saw the training set given*, it's that there is some missing or non relative variables(registers), because some variables have no values at all, some others may present a value registered but not in all rows, and some others have NA values. So the first preprocess made was directly in the data source, eliminating those fields with this characteristics. The training set variables left with 58 of them, including the variable wich try to predict(classe).
The important preprocess technique made come later. Using principal components analysis (PCA) for picking those reamian variables that been correlated enough to bring us the best combination of significant predictors.

**The Cross validation and expected out of sample error**

The final model use the "PCA" method that give us the combination of the most important predictors that reduce the noise and maintain the accuracy. Combinating the sesgo of data training set and applying the PCA method to it come out whit 36 significant predictors of the total of 57 possible ones. The data training was split in 70% for training and 30% for testing.

Whit all this, it was expected an out of sample error very small, (cause seeing no miss classification, will probably and surely would be wrong), estimating an error of 0.0042. In ej. with 20 predictions made we were wrong in 1.68 of them.

**Model and Confusion Matrix**

                                  **Random Forest**

13737 samples
   57 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' 

Pre-processing: principal
 component signal extraction,
 scaled, centered 
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 

Resampling results across tuning parameters:

         mtry Accuracy    Kappa    AccuracySD    KappaSD    
         
         2     0.974    0.967       0.00217      0.00273
         
         40    0.962    0.952       0.0052       0.00656
         
         79    0.962    0.952       0.00534      0.00674    

  
Accuracy was used to select
 the optimal model using  the
 largest value.
The final value used for the model
 was mtry = 2.


                                  **Confusion Matrix and Statistics**

Prediction    A    B    C    D    E

         A 1671    5    0    0    0
         
         B    2 1133    6    0    0
         
         C    1    1 1020    8    0
         
         D    0    0    0  955    1
         
         E    0    0    0    1 1081

Overall Statistics
                                          
               Accuracy : 0.9958 
               
                 95% CI : (0.9937, 0.9972)
                 
    No Information Rate : 0.2845          
    
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9946
                  
 Mcnemar's Test P-Value : NA 


*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. at <http://groupware.les.inf.puc-rio.br/har.>